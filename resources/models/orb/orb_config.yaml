# ORB model configuration for Equitrain

# Data paths
train_file: data/train.h5
valid_file: data/valid.h5
output_dir: orb_training

# Model configuration
model_wrapper: orb
model: null  # Will use pretrained ORB model from zoo

# ORB-specific settings
model_variant: direct  # 'direct' or 'conservative'
enable_zbl: false      # Enable ZBL repulsion for high-Z elements (Z > 56)

# Training parameters
epochs: 100
batch_size: 32
lr: 0.001
verbose: 1
tqdm: true

# Loss weights (ORB defaults: 0.01 × energy + 1.0 × forces + 0.1 × stress)
energy_weight: 0.01
forces_weight: 1.0
stress_weight: 0.1

# Loss function
loss_type: mse
loss_energy_per_atom: true

# Optimizer
optimizer: Adam
scheduler: ReduceLROnPlateau
scheduler_patience: 10
scheduler_factor: 0.5
scheduler_min_lr: 0.00001

# Distributed training
accelerator: auto
devices: auto
precision: 16  # ORB v3 models were trained in FP16 and tolerate AMP

# Validation
validation_frequency: 5  # Compute Matbench-MAE every 5 epochs

# Memory optimization
pin_memory: true  # Pin memory to avoid graph recompilation lag
