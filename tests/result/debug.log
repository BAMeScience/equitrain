2024-10-21 17:16:59,537 - train_fabric.py:234 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False)
2024-10-21 17:16:59,604 - train_fabric.py:234 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-21 17:24:31,908 - train_fabric.py:234 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-21 17:24:31,982 - train_fabric.py:234 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-21 17:30:13,329 - train_fabric.py:234 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-21 17:30:13,400 - train_fabric.py:234 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-21 17:30:17,989 - train_fabric.py:238 - Training finalized with status: failed
2024-10-21 17:33:53,747 - train_fabric.py:236 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-21 17:33:53,830 - train_fabric.py:236 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-21 17:33:58,681 - train_fabric.py:240 - Training finalized with status: failed
2024-10-21 17:34:57,763 - train_fabric.py:237 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-21 17:34:57,832 - train_fabric.py:237 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-21 17:35:02,356 - train_fabric.py:241 - Training finalized with status: failed
2024-10-22 10:23:35,149 - train_fabric.py:237 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 10:23:35,212 - train_fabric.py:237 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 10:23:43,006 - train_fabric.py:241 - Training finalized with status: failed
2024-10-22 10:25:51,135 - train_fabric.py:238 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 10:25:51,186 - train_fabric.py:238 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 10:25:59,213 - train_fabric.py:242 - Training finalized with status: failed
2024-10-22 12:28:23,415 - train_fabric.py:238 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 12:28:23,487 - train_fabric.py:238 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 12:28:31,311 - train_fabric.py:242 - Training finalized with status: failed
2024-10-22 12:31:46,008 - train_fabric.py:238 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 12:31:46,063 - train_fabric.py:238 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 12:31:53,841 - train_fabric.py:242 - Training finalized with status: failed
2024-10-22 12:33:54,060 - train_fabric.py:237 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 12:33:54,131 - train_fabric.py:237 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 12:34:03,561 - train_fabric.py:237 - Epoch [0] Val -- loss: 3211.06289, loss_e: 0.61351, loss_f: 4013.67505
2024-10-22 12:34:05,576 - train_fabric.py:237 - Epoch [0] Val -- loss: 6166.05467, loss_e: 0.96955, loss_f: 7707.32593
2024-10-22 12:34:05,580 - train_fabric.py:237 - Epoch [0] Train -- loss: 6166.05467, loss_e: 0.96955, loss_f: 7707.32593
2024-10-22 12:34:05,581 - train_fabric.py:241 - Training finalized with status: failed
2024-10-22 12:40:35,409 - train_fabric.py:237 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 12:40:35,470 - train_fabric.py:237 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 12:40:44,987 - train_fabric.py:241 - Training finalized with status: failed
2024-10-22 13:59:45,506 - train_fabric.py:237 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 13:59:45,559 - train_fabric.py:237 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 13:59:55,116 - train_fabric.py:237 - Epoch [0] Val -- loss: 6408.63687, loss_e: 0.61354, loss_f: 8010.64258
2024-10-22 13:59:57,210 - train_fabric.py:237 - Epoch [0] Val -- loss: 1994.80348, loss_e: 1.08741, loss_f: 2493.23242
2024-10-22 13:59:57,211 - train_fabric.py:241 - Training finalized with status: failed
2024-10-22 14:01:38,258 - train_fabric.py:237 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 14:01:38,312 - train_fabric.py:237 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 14:01:47,774 - train_fabric.py:241 - Training finalized with status: failed
2024-10-22 14:16:25,583 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 14:16:25,646 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 14:16:34,724 - train_fabric.py:260 - Epoch [0] Val -- loss: 2699.54649, loss_e: 0.61331, loss_f: 3374.27979
2024-10-22 14:16:36,804 - train_fabric.py:260 - Epoch [0] Val -- loss: 2839.89207, loss_e: 9.56839, loss_f: 3547.47290
2024-10-22 14:16:36,805 - train_fabric.py:264 - Training finalized with status: failed
2024-10-22 14:20:22,190 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 14:20:22,241 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 14:20:31,255 - train_fabric.py:260 - Epoch [0] Val -- loss: 2688.08453, loss_e: 0.61309, loss_f: 3359.95239
2024-10-22 14:20:33,316 - train_fabric.py:260 - Epoch [0] Val -- loss: 4552.11808, loss_e: 4.97882, loss_f: 5688.90283
2024-10-22 14:20:33,317 - train_fabric.py:278 - val_loss: 4102.924512711167 at step 0
2024-10-22 14:20:33,317 - train_fabric.py:278 - loss_e: 9.344243243336678 at step 0
2024-10-22 14:20:33,317 - train_fabric.py:278 - loss_f: 5126.3193359375 at step 0
2024-10-22 14:20:33,317 - train_fabric.py:278 - epoch: 0 at step 0
2024-10-22 14:20:33,321 - train_fabric.py:260 - Epoch [0] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 14:20:33,570 - train_fabric.py:264 - Training finalized with status: failed
2024-10-22 15:03:36,432 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 15:03:36,487 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 15:03:45,775 - train_fabric.py:260 - Epoch [0] Val -- loss: 4469.42003, loss_e: 0.61332, loss_f: 5586.62158
2024-10-22 15:03:47,799 - train_fabric.py:260 - Epoch [0] Val -- loss: 2835.63817, loss_e: 6.40924, loss_f: 3542.94543
2024-10-22 15:03:47,800 - train_fabric.py:274 - val_loss: 3273.668328481913 at step 0
2024-10-22 15:03:47,800 - train_fabric.py:274 - loss_e: 12.205167800188065 at step 0
2024-10-22 15:03:47,800 - train_fabric.py:274 - loss_f: 4089.0341796875 at step 0
2024-10-22 15:03:47,800 - train_fabric.py:274 - epoch: 0 at step 0
2024-10-22 15:03:47,804 - train_fabric.py:260 - Epoch [0] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:03:48,067 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fdf4cc0fe80>
2024-10-22 15:03:48,068 - train_fabric.py:263 - Training finalized with status: failed
2024-10-22 15:06:53,898 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 15:06:53,953 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 15:07:03,061 - train_fabric.py:260 - Epoch [0] Val -- loss: 2268.32453, loss_e: 0.61311, loss_f: 2835.25220
2024-10-22 15:07:05,111 - train_fabric.py:260 - Epoch [0] Val -- loss: 4754.60798, loss_e: 4.38330, loss_f: 5942.16406
2024-10-22 15:07:05,112 - train_fabric.py:274 - val_loss: 6224.032946629823 at step 0
2024-10-22 15:07:05,112 - train_fabric.py:274 - loss_e: 8.153014399111271 at step 0
2024-10-22 15:07:05,112 - train_fabric.py:274 - loss_f: 7778.0029296875 at step 0
2024-10-22 15:07:05,112 - train_fabric.py:274 - epoch: 0 at step 0
2024-10-22 15:07:05,116 - train_fabric.py:260 - Epoch [0] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:07:05,374 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f4ea327e950>
2024-10-22 15:07:05,374 - train_fabric.py:263 - Training finalized with status: failed
2024-10-22 15:09:10,889 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 15:09:10,949 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 15:09:19,845 - train_fabric.py:260 - Epoch [0] Val -- loss: 2343.32214, loss_e: 0.61338, loss_f: 2928.99927
2024-10-22 15:09:21,859 - train_fabric.py:260 - Epoch [0] Val -- loss: 4065.21499, loss_e: 2.85655, loss_f: 5080.80457
2024-10-22 15:09:21,860 - train_fabric.py:274 - val_loss: 5660.278281778842 at step 0
2024-10-22 15:09:21,860 - train_fabric.py:274 - loss_e: 5.099904987961054 at step 0
2024-10-22 15:09:21,860 - train_fabric.py:274 - loss_f: 7074.07275390625 at step 0
2024-10-22 15:09:21,860 - train_fabric.py:274 - epoch: 0 at step 0
2024-10-22 15:09:21,864 - train_fabric.py:260 - Epoch [0] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:09:22,107 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f961171e500>
2024-10-22 15:09:22,107 - train_fabric.py:263 - Training finalized with status: failed
2024-10-22 15:10:57,630 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 15:10:57,685 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 15:11:06,385 - train_fabric.py:260 - Epoch [0] Val -- loss: 2146.19146, loss_e: 0.61305, loss_f: 2682.58594
2024-10-22 15:11:08,371 - train_fabric.py:260 - Epoch [0] Val -- loss: 3868.30994, loss_e: 6.27689, loss_f: 4833.81824
2024-10-22 15:11:08,372 - train_fabric.py:274 - val_loss: 5297.429075866938 at step 0
2024-10-22 15:11:08,372 - train_fabric.py:274 - loss_e: 11.940301209688187 at step 0
2024-10-22 15:11:08,372 - train_fabric.py:274 - loss_f: 6618.80078125 at step 0
2024-10-22 15:11:08,372 - train_fabric.py:274 - epoch: 0 at step 0
2024-10-22 15:11:08,383 - train_fabric.py:260 - Epoch [0] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:11:08,642 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f2c937ea500>
2024-10-22 15:11:08,642 - train_fabric.py:263 - Training finalized with status: failed
2024-10-22 15:11:45,463 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 15:11:45,515 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 15:11:54,406 - train_fabric.py:260 - Epoch [0] Val -- loss: 2247.69443, loss_e: 0.61329, loss_f: 2809.46460
2024-10-22 15:11:56,427 - train_fabric.py:260 - Epoch [0] Val -- loss: 3806.58379, loss_e: 5.05091, loss_f: 4756.96680
2024-10-22 15:11:56,428 - train_fabric.py:274 - val_loss: 5165.672527115047 at step 0
2024-10-22 15:11:56,428 - train_fabric.py:274 - loss_e: 9.48812385648489 at step 0
2024-10-22 15:11:56,428 - train_fabric.py:274 - loss_f: 6454.71826171875 at step 0
2024-10-22 15:11:56,428 - train_fabric.py:274 - epoch: 0 at step 0
2024-10-22 15:11:56,432 - train_fabric.py:260 - Epoch [0] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:11:56,684 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f5507896950>
2024-10-22 15:11:59,053 - train_fabric.py:260 - Epoch [1] Val -- loss: 11544.67674, loss_e: 5.90447, loss_f: 14429.36987
2024-10-22 15:11:59,053 - train_fabric.py:274 - val_loss: 17774.407608762383 at step 1
2024-10-22 15:11:59,053 - train_fabric.py:274 - loss_e: 2.321246936917305 at step 1
2024-10-22 15:11:59,054 - train_fabric.py:274 - loss_f: 22217.427734375 at step 1
2024-10-22 15:11:59,054 - train_fabric.py:274 - epoch: 1 at step 1
2024-10-22 15:11:59,057 - train_fabric.py:260 - Epoch [1] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:01,421 - train_fabric.py:260 - Epoch [2] Val -- loss: 13513.44980, loss_e: 1.35544, loss_f: 16891.47314
2024-10-22 15:12:01,422 - train_fabric.py:274 - val_loss: 10181.326091920491 at step 2
2024-10-22 15:12:01,422 - train_fabric.py:274 - loss_e: 0.39022522745653987 at step 2
2024-10-22 15:12:01,422 - train_fabric.py:274 - loss_f: 12726.560546875 at step 2
2024-10-22 15:12:01,422 - train_fabric.py:274 - epoch: 2 at step 2
2024-10-22 15:12:01,426 - train_fabric.py:260 - Epoch [2] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:03,258 - train_fabric.py:260 - Epoch [3] Val -- loss: 6760.59889, loss_e: 2.06304, loss_f: 8450.23267
2024-10-22 15:12:03,259 - train_fabric.py:274 - val_loss: 4765.455092606693 at step 3
2024-10-22 15:12:03,259 - train_fabric.py:274 - loss_e: 3.737865377217531 at step 3
2024-10-22 15:12:03,259 - train_fabric.py:274 - loss_f: 5955.88427734375 at step 3
2024-10-22 15:12:03,259 - train_fabric.py:274 - epoch: 3 at step 3
2024-10-22 15:12:03,263 - train_fabric.py:260 - Epoch [3] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:03,802 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f5507896950>
2024-10-22 15:12:05,034 - train_fabric.py:260 - Epoch [4] Val -- loss: 7596.66399, loss_e: 3.17958, loss_f: 9495.03516
2024-10-22 15:12:05,035 - train_fabric.py:274 - val_loss: 10842.004780929536 at step 4
2024-10-22 15:12:05,035 - train_fabric.py:274 - loss_e: 2.621560897678137 at step 4
2024-10-22 15:12:05,036 - train_fabric.py:274 - loss_f: 13551.8505859375 at step 4
2024-10-22 15:12:05,036 - train_fabric.py:274 - epoch: 4 at step 4
2024-10-22 15:12:05,040 - train_fabric.py:260 - Epoch [4] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:06,360 - train_fabric.py:260 - Epoch [5] Val -- loss: 14237.48487, loss_e: 1.52592, loss_f: 17796.47412
2024-10-22 15:12:06,361 - train_fabric.py:274 - val_loss: 17662.080177602173 at step 5
2024-10-22 15:12:06,361 - train_fabric.py:274 - loss_e: 0.43018488585948944 at step 5
2024-10-22 15:12:06,361 - train_fabric.py:274 - loss_f: 22077.4921875 at step 5
2024-10-22 15:12:06,361 - train_fabric.py:274 - epoch: 5 at step 5
2024-10-22 15:12:06,365 - train_fabric.py:260 - Epoch [5] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:07,617 - train_fabric.py:260 - Epoch [6] Val -- loss: 11359.43741, loss_e: 0.88576, loss_f: 14199.07520
2024-10-22 15:12:07,618 - train_fabric.py:274 - val_loss: 8260.646170226299 at step 6
2024-10-22 15:12:07,618 - train_fabric.py:274 - loss_e: 1.341202693991363 at step 6
2024-10-22 15:12:07,618 - train_fabric.py:274 - loss_f: 10325.47265625 at step 6
2024-10-22 15:12:07,618 - train_fabric.py:274 - epoch: 6 at step 6
2024-10-22 15:12:07,622 - train_fabric.py:260 - Epoch [6] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:08,973 - train_fabric.py:260 - Epoch [7] Val -- loss: 12272.66577, loss_e: 1.36107, loss_f: 15340.49170
2024-10-22 15:12:08,974 - train_fabric.py:274 - val_loss: 13349.668757584504 at step 7
2024-10-22 15:12:08,974 - train_fabric.py:274 - loss_e: 1.3808972975239158 at step 7
2024-10-22 15:12:08,974 - train_fabric.py:274 - loss_f: 16686.740234375 at step 7
2024-10-22 15:12:08,974 - train_fabric.py:274 - epoch: 7 at step 7
2024-10-22 15:12:08,978 - train_fabric.py:260 - Epoch [7] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:10,215 - train_fabric.py:260 - Epoch [8] Val -- loss: 7899.81850, loss_e: 1.38620, loss_f: 9874.42627
2024-10-22 15:12:10,215 - train_fabric.py:274 - val_loss: 8961.772439824044 at step 8
2024-10-22 15:12:10,215 - train_fabric.py:274 - loss_e: 1.3914959952235222 at step 8
2024-10-22 15:12:10,215 - train_fabric.py:274 - loss_f: 11201.8671875 at step 8
2024-10-22 15:12:10,215 - train_fabric.py:274 - epoch: 8 at step 8
2024-10-22 15:12:10,219 - train_fabric.py:260 - Epoch [8] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:11,455 - train_fabric.py:260 - Epoch [9] Val -- loss: 5945.04079, loss_e: 1.38195, loss_f: 7430.95532
2024-10-22 15:12:11,456 - train_fabric.py:274 - val_loss: 4946.7670883582905 at step 9
2024-10-22 15:12:11,456 - train_fabric.py:274 - loss_e: 1.3720628852024674 at step 9
2024-10-22 15:12:11,456 - train_fabric.py:274 - loss_f: 6183.11572265625 at step 9
2024-10-22 15:12:11,456 - train_fabric.py:274 - epoch: 9 at step 9
2024-10-22 15:12:11,462 - train_fabric.py:260 - Epoch [9] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:12:11,674 - train_fabric.py:263 - Training finalized with status: success
2024-10-22 15:16:11,590 - train_fabric.py:260 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 15:16:11,643 - train_fabric.py:260 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 15:16:20,708 - train_fabric.py:260 - Epoch [0] Val -- loss: 2502.06577, loss_e: 0.61327, loss_f: 3127.42871
2024-10-22 15:16:23,360 - train_fabric.py:260 - Epoch [0] Val -- loss: 3348.03535, loss_e: 2.86902, loss_f: 4184.32678
2024-10-22 15:16:23,360 - train_fabric.py:274 - val_loss: 4164.96196205616 at step 0
2024-10-22 15:16:23,360 - train_fabric.py:274 - loss_e: 5.124751687049866 at step 0
2024-10-22 15:16:23,361 - train_fabric.py:274 - loss_f: 5204.92041015625 at step 0
2024-10-22 15:16:23,361 - train_fabric.py:274 - epoch: 0 at step 0
2024-10-22 15:16:23,364 - train_fabric.py:260 - Epoch [0] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:23,641 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fea0ff05870>
2024-10-22 15:16:25,512 - train_fabric.py:260 - Epoch [1] Val -- loss: 5689.98363, loss_e: 3.57536, loss_f: 7111.58569
2024-10-22 15:16:25,512 - train_fabric.py:274 - val_loss: 7412.662962956727 at step 1
2024-10-22 15:16:25,513 - train_fabric.py:274 - loss_e: 2.0257522836327553 at step 1
2024-10-22 15:16:25,513 - train_fabric.py:274 - loss_f: 9265.322265625 at step 1
2024-10-22 15:16:25,513 - train_fabric.py:274 - epoch: 1 at step 1
2024-10-22 15:16:25,517 - train_fabric.py:260 - Epoch [1] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:27,375 - train_fabric.py:260 - Epoch [2] Val -- loss: 8061.62696, loss_e: 1.93971, loss_f: 10076.54834
2024-10-22 15:16:27,376 - train_fabric.py:274 - val_loss: 8450.54512230009 at step 2
2024-10-22 15:16:27,376 - train_fabric.py:274 - loss_e: 1.851588062942028 at step 2
2024-10-22 15:16:27,376 - train_fabric.py:274 - loss_f: 10562.7177734375 at step 2
2024-10-22 15:16:27,376 - train_fabric.py:274 - epoch: 2 at step 2
2024-10-22 15:16:27,380 - train_fabric.py:260 - Epoch [2] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:28,675 - train_fabric.py:260 - Epoch [3] Val -- loss: 6807.25543, loss_e: 2.57721, loss_f: 8508.42456
2024-10-22 15:16:28,676 - train_fabric.py:274 - val_loss: 4007.567280327529 at step 3
2024-10-22 15:16:28,676 - train_fabric.py:274 - loss_e: 3.3027102313935757 at step 3
2024-10-22 15:16:28,676 - train_fabric.py:274 - loss_f: 5008.63330078125 at step 3
2024-10-22 15:16:28,676 - train_fabric.py:274 - epoch: 3 at step 3
2024-10-22 15:16:28,681 - train_fabric.py:260 - Epoch [3] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:28,995 - train_fabric.py:279 - Checkpoint saved at <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fea0ff05870>
2024-10-22 15:16:30,313 - train_fabric.py:260 - Epoch [4] Val -- loss: 7704.02240, loss_e: 4.60787, loss_f: 9628.87573
2024-10-22 15:16:30,314 - train_fabric.py:274 - val_loss: 12147.438445584476 at step 4
2024-10-22 15:16:30,314 - train_fabric.py:274 - loss_e: 5.9129310473799706 at step 4
2024-10-22 15:16:30,315 - train_fabric.py:274 - loss_f: 15182.8193359375 at step 4
2024-10-22 15:16:30,315 - train_fabric.py:274 - epoch: 4 at step 4
2024-10-22 15:16:30,320 - train_fabric.py:260 - Epoch [4] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:31,629 - train_fabric.py:260 - Epoch [5] Val -- loss: 11553.84929, loss_e: 6.88561, loss_f: 14440.58984
2024-10-22 15:16:31,630 - train_fabric.py:274 - val_loss: 10451.838324409724 at step 5
2024-10-22 15:16:31,630 - train_fabric.py:274 - loss_e: 7.858614236116409 at step 5
2024-10-22 15:16:31,630 - train_fabric.py:274 - loss_f: 13062.8330078125 at step 5
2024-10-22 15:16:31,630 - train_fabric.py:274 - epoch: 5 at step 5
2024-10-22 15:16:31,634 - train_fabric.py:260 - Epoch [5] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:32,916 - train_fabric.py:260 - Epoch [6] Val -- loss: 121360.96562, loss_e: 42.17675, loss_f: 151690.65674
2024-10-22 15:16:32,917 - train_fabric.py:274 - val_loss: 230551.65835053922 at step 6
2024-10-22 15:16:32,917 - train_fabric.py:274 - loss_e: 76.49487769603729 at step 6
2024-10-22 15:16:32,917 - train_fabric.py:274 - loss_f: 288170.4375 at step 6
2024-10-22 15:16:32,918 - train_fabric.py:274 - epoch: 6 at step 6
2024-10-22 15:16:32,922 - train_fabric.py:260 - Epoch [6] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:34,232 - train_fabric.py:260 - Epoch [7] Val -- loss: 158885.35328, loss_e: 77.19611, loss_f: 198587.39062
2024-10-22 15:16:34,233 - train_fabric.py:274 - val_loss: 154449.0163572669 at step 7
2024-10-22 15:16:34,233 - train_fabric.py:274 - loss_e: 77.89428633451462 at step 7
2024-10-22 15:16:34,233 - train_fabric.py:274 - loss_f: 193041.796875 at step 7
2024-10-22 15:16:34,233 - train_fabric.py:274 - epoch: 7 at step 7
2024-10-22 15:16:34,237 - train_fabric.py:260 - Epoch [7] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:35,538 - train_fabric.py:260 - Epoch [8] Val -- loss: 109129.52327, loss_e: 78.08508, loss_f: 136392.37891
2024-10-22 15:16:35,539 - train_fabric.py:274 - val_loss: 71833.56170053482 at step 8
2024-10-22 15:16:35,539 - train_fabric.py:274 - loss_e: 78.27725267410278 at step 8
2024-10-22 15:16:35,539 - train_fabric.py:274 - loss_f: 89772.3828125 at step 8
2024-10-22 15:16:35,539 - train_fabric.py:274 - epoch: 8 at step 8
2024-10-22 15:16:35,544 - train_fabric.py:260 - Epoch [8] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:36,864 - train_fabric.py:260 - Epoch [9] Val -- loss: 96924.91672, loss_e: 77.95272, loss_f: 121136.65625
2024-10-22 15:16:36,865 - train_fabric.py:274 - val_loss: 54459.16254245043 at step 9
2024-10-22 15:16:36,865 - train_fabric.py:274 - loss_e: 77.62911850214005 at step 9
2024-10-22 15:16:36,865 - train_fabric.py:274 - loss_f: 68054.546875 at step 9
2024-10-22 15:16:36,865 - train_fabric.py:274 - epoch: 9 at step 9
2024-10-22 15:16:36,869 - train_fabric.py:260 - Epoch [9] Train -- loss: 0.00000, loss_e: 0.00000, loss_f: 0.00000
2024-10-22 15:16:37,105 - train_fabric.py:263 - Training finalized with status: success
2024-10-22 15:17:24,560 - train.py:82 - Namespace(train_file='tests/data/train.h5', valid_file='tests/data/valid.h5', test_file=None, statistics_file='tests/data/statistics.json', output_dir='tests/result', epochs=10, batch_size=64, batch_edge_limit=0, eval_batch_size=24, model='v1', alpha_drop=0.0, proj_drop=0.0, out_drop=0.0, drop_path_rate=0.0, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.005, sched='plateau', lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=0.01, min_lr=1e-06, decay_epochs=30, warmup_epochs=0, cooldown_epochs=0, patience_epochs=2, decay_rate=0.5, print_freq=100, energy_weight=0.2, force_weight=0.8, stress_weight=0.0, seed=1, workers=4, pin_mem=True, shuffle=True, load_checkpoint=None, load_checkpoint_model=None, evaluate=False, mixed_precision=False)
2024-10-22 15:17:24,616 - train.py:82 - Using r_max=4.5 from statistics file `tests/data/statistics.json`
2024-10-22 15:17:32,256 - train.py:82 - Number of params: 8725251
2024-10-22 15:17:33,943 - train.py:82 - Epoch [   0] Val   -- loss: 11325.22348, loss_e: 0.60960, loss_f: 14156.37695
2024-10-22 15:17:35,443 - train.py:82 - Epoch [   1][     0/1] -- loss: 9183.27434, loss_e: 0.61000, loss_f: 11478.94043, time/step=1483ms, lr=1.00e-02
2024-10-22 15:17:36,182 - train.py:82 - Validation error decreased. Saving model to `best_val_epochs@1_e@10152.8962`...
2024-10-22 15:17:36,339 - train.py:82 - Epoch [   1] Train -- loss: 9183.27434, loss_e: 0.61000, loss_f: 11478.94043, Time: 2.39s
2024-10-22 15:17:36,339 - train.py:82 - Epoch [   1] Val   -- loss: 10152.89619, loss_e: 12.00538, loss_f: 12688.11816
2024-10-22 15:17:37,667 - train.py:82 - Epoch [   2][     0/1] -- loss: 11826.04186, loss_e: 12.00616, loss_f: 14779.55078, time/step=1322ms, lr=1.00e-02
2024-10-22 15:17:38,277 - train.py:82 - Epoch [   2] Train -- loss: 11826.04186, loss_e: 12.00616, loss_f: 14779.55078, Time: 1.94s
2024-10-22 15:17:38,279 - train.py:82 - Epoch [   2] Val   -- loss: 28895.89633, loss_e: 14.13988, loss_f: 36116.33594
2024-10-22 15:17:39,688 - train.py:82 - Epoch [   3][     0/1] -- loss: 32670.97826, loss_e: 14.13934, loss_f: 40835.18750, time/step=1402ms, lr=1.00e-02
2024-10-22 15:17:40,265 - train.py:82 - Epoch [   3] Train -- loss: 32670.97826, loss_e: 14.13934, loss_f: 40835.18750, Time: 1.99s
2024-10-22 15:17:40,266 - train.py:82 - Epoch [   3] Val   -- loss: 26383.99753, loss_e: 9.48961, loss_f: 32977.62500
2024-10-22 15:17:41,099 - train.py:82 - Epoch [   4][     0/1] -- loss: 16884.77484, loss_e: 9.48945, loss_f: 21103.59570, time/step=826ms, lr=1.00e-02
2024-10-22 15:17:41,662 - train.py:82 - Epoch [   4] Train -- loss: 16884.77484, loss_e: 9.48945, loss_f: 21103.59570, Time: 1.40s
2024-10-22 15:17:41,662 - train.py:82 - Epoch [   4] Val   -- loss: 14849.97759, loss_e: 5.58129, loss_f: 18561.07617
2024-10-22 15:17:42,507 - train.py:82 - Epoch [   5][     0/1] -- loss: 17450.42082, loss_e: 5.58066, loss_f: 21811.63086, time/step=838ms, lr=1.00e-02
2024-10-22 15:17:43,113 - train.py:82 - Epoch [   5] Train -- loss: 17450.42082, loss_e: 5.58066, loss_f: 21811.63086, Time: 1.45s
2024-10-22 15:17:43,114 - train.py:82 - Epoch [   5] Val   -- loss: 16095.42249, loss_e: 5.12027, loss_f: 20117.99805
2024-10-22 15:17:43,944 - train.py:82 - Epoch [   6][     0/1] -- loss: 13187.78382, loss_e: 5.12028, loss_f: 16483.44922, time/step=823ms, lr=1.00e-02
2024-10-22 15:17:44,579 - train.py:82 - Epoch [   6] Train -- loss: 13187.78382, loss_e: 5.12028, loss_f: 16483.44922, Time: 1.46s
2024-10-22 15:17:44,579 - train.py:82 - Epoch [   6] Val   -- loss: 86025.65843, loss_e: 7.08121, loss_f: 107530.30469
2024-10-22 15:17:45,390 - train.py:82 - Epoch [   7][     0/1] -- loss: 117615.90848, loss_e: 7.08147, loss_f: 147018.10938, time/step=803ms, lr=1.00e-02
2024-10-22 15:17:45,968 - train.py:82 - Epoch [   7] Train -- loss: 117615.90848, loss_e: 7.08147, loss_f: 147018.10938, Time: 1.39s
2024-10-22 15:17:45,969 - train.py:82 - Epoch [   7] Val   -- loss: 78463.19315, loss_e: 13.34856, loss_f: 98075.65625
2024-10-22 15:17:46,829 - train.py:82 - Epoch [   8][     0/1] -- loss: 53101.61884, loss_e: 13.34810, loss_f: 66373.68750, time/step=849ms, lr=1.00e-02
2024-10-22 15:17:47,457 - train.py:82 - Epoch [   8] Train -- loss: 53101.61884, loss_e: 13.34810, loss_f: 66373.68750, Time: 1.49s
2024-10-22 15:17:47,458 - train.py:82 - Epoch [   8] Val   -- loss: 57836.24813, loss_e: 17.45161, loss_f: 72290.94531
2024-10-22 15:17:48,261 - train.py:82 - Epoch [   9][     0/1] -- loss: 48608.38099, loss_e: 17.45181, loss_f: 60756.11328, time/step=795ms, lr=1.00e-02
2024-10-22 15:17:48,847 - train.py:82 - Epoch [   9] Train -- loss: 48608.38099, loss_e: 17.45181, loss_f: 60756.11328, Time: 1.39s
2024-10-22 15:17:48,848 - train.py:82 - Epoch [   9] Val   -- loss: 160935.21808, loss_e: 17.96539, loss_f: 201164.53125
2024-10-22 15:17:49,654 - train.py:82 - Epoch [  10][     0/1] -- loss: 45394.02669, loss_e: 17.96549, loss_f: 56738.03906, time/step=800ms, lr=1.00e-02
2024-10-22 15:17:50,256 - train.py:82 - Epoch [  10] Train -- loss: 45394.02669, loss_e: 17.96549, loss_f: 56738.03906, Time: 1.41s
2024-10-22 15:17:50,257 - train.py:82 - Epoch [  10] Val   -- loss: 11838.34396, loss_e: 15.95321, loss_f: 14793.94141
